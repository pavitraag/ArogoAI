# -*- coding: utf-8 -*-
"""ArogoAI2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HS4NdGKiwSpxcwfgWd0hIEB6itmR3UOU
"""

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd
import numpy as np
import xgboost as xgb
import joblib
import os
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import shap
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score


# Load Dataset (Ensure the dataset is manually downloaded and placed in the working directory)
try:
    df = pd.read_csv("/content/drive/MyDrive/depression_anxiety_data.csv")
    print("Dataset loaded successfully!")
except FileNotFoundError:
    print("Error: survey.csv not found. Please check the file path.")
    exit()

df.head()

# prompt: Using dataframe df: where depression_diagnosis is true

# Filter the dataframe to include only rows where depression_diagnosis is True (1).
df_depression = df[df['depression_diagnosis'] == 1]

# Display the filtered dataframe.
df_depression

df.columns

df.tail()

df.info()

df.dtypes

df.select_dtypes('object').describe().transpose()

df.drop('id', axis=1).select_dtypes('number').describe().transpose()

df.isna().any()

print(df['depression_diagnosis'].isna().sum())

df['depression_diagnosis'] = df['depression_diagnosis'].fillna(0)

df['depression_diagnosis'] = df['depression_diagnosis'].astype(int)

print(df['anxiety_diagnosis'].isna().sum())

df['anxiety_diagnosis'] = df['anxiety_diagnosis'].fillna(0)

df['anxiety_diagnosis'] = df['anxiety_diagnosis'].astype(int)

# Handling missing values
categorical_cols = ['depression_severity', 'depressiveness', 'suicidal',
                    'depression_treatment', 'anxiety_severity', 'anxiousness',
                     'anxiety_treatment', 'sleepiness']
numerical_cols = ['epworth_score']

df[categorical_cols] = df[categorical_cols].fillna("Unknown")
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())

# Convert boolean-like categorical columns to numerical values (0/1)
binary_cols = ['depressiveness', 'suicidal', 'depression_treatment',
               'anxiousness', 'anxiety_treatment', 'sleepiness']
df[binary_cols] = df[binary_cols].applymap(lambda x: 1 if x == "True" else 0)

# Check unique values to find unexpected categories
print(df['depression_severity'].unique())
print(df['anxiety_severity'].unique())

# Define severity mapping, handling all variations
severity_mapping = {
    'None-minimal': 0, 'none': 0, 'None': 0,
    'Mild': 1, 'Moderate': 2, 'Moderately severe': 3, 'Severe': 4,
    'Unknown': 0, '0': 0   # Ensure 'Unknown' is mapped to 0
}

# Apply the mapping to replace categorical values with numeric values
df['depression_severity'] = df['depression_severity'].replace(severity_mapping)
df['anxiety_severity'] = df['anxiety_severity'].replace(severity_mapping)

# Convert to integers after ensuring proper mapping
df[['depression_severity', 'anxiety_severity']] = df[['depression_severity', 'anxiety_severity']].astype(int)

# Verify changes
print(df[['depression_severity', 'anxiety_severity']].dtypes)  # Should be int
print(df['depression_severity'].unique())
print(df['anxiety_severity'].unique())

# Encode categorical columns with LabelEncoder
label_cols = ['gender', 'who_bmi']
encoder = LabelEncoder()
for col in label_cols:
    df[col] = encoder.fit_transform(df[col])

# Drop ID column
df.drop(columns=['id'], inplace=True)

df['epworth_score'] = df['epworth_score'].astype(int)

df['bmi'] = df['bmi'].astype(int)

df.info()

# Exploratory Data Analysis (Correlation Heatmap)
corr_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Heatmap of Mental Health Features")
plt.show()

from sklearn.feature_selection import SelectKBest, f_classif

X = df.drop(columns=['depression_diagnosis', 'anxiety_diagnosis'])
y = df[['depression_diagnosis', 'anxiety_diagnosis']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Feature Selection using ANOVA F-test (for depression diagnosis)
selector = SelectKBest(score_func=f_classif, k=10)
X_new = selector.fit_transform(X_train, y_train['depression_diagnosis'])
selected_features = X_train.columns[selector.get_support()]
print("Selected Features:", selected_features)

#Shows pairwise relationships between numerical features.
sns.pairplot(X_train)
plt.show()

#Shows how data is distributed across different features
X_train.hist(figsize=(12, 8), bins=20, edgecolor="black")
plt.suptitle("Feature Distributions", fontsize=14)
plt.show()

# Normalize only numerical features
scaler = StandardScaler()
X_train[selected_features] = scaler.fit_transform(X_train[selected_features])
X_test[selected_features] = scaler.transform(X_test[selected_features])

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report

# Train models
rf_model = RandomForestClassifier(random_state=42)
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

rf_model.fit(X_train[selected_features], y_train['depression_diagnosis'])
xgb_model.fit(X_train[selected_features], y_train['depression_diagnosis'])

# Predictions
rf_preds = rf_model.predict(X_test[selected_features])
xgb_preds = xgb_model.predict(X_test[selected_features])

# Evaluation function
def evaluate_model(y_true, y_pred, model_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    # Check if y_true has multiple classes before computing ROC-AUC
    if len(set(y_true)) > 1:
        roc_auc = roc_auc_score(y_true, y_pred)
    else:
        roc_auc = "N/A (Only one class present)"

    print(f"{model_name} Performance:")
    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1-score:", f1)
    print("ROC-AUC:", roc_auc)
    print("Classification Report:\n", classification_report(y_true, y_pred))
    print("-" * 50)

    return accuracy, precision, recall, f1, roc_auc

rf_metrics = evaluate_model(y_test['depression_diagnosis'], rf_preds, "Random Forest")
xgb_metrics = evaluate_model(y_test['depression_diagnosis'], xgb_preds, "XGBoost")

# Determine the best model
if rf_metrics[3] > xgb_metrics[3]:
    best_model = "Random Forest"
    bm=rf_model
elif rf_metrics[3] < xgb_metrics[3]:
    best_model = "XGBoost"
    bm=xgb_model
else:
    best_model = "Both models perform equally"

print(f"The best performing model is: {best_model}")

import pickle

# Assuming `best_model` is your trained model
with open('bm1.pkl', 'wb') as file:
    pickle.dump(bm, file)

# Save Model and Preprocessing Artifacts
joblib.dump(bm, "bm1.pkl")
joblib.dump(scaler, "scaler.pkl")
print("Model and artifacts saved successfully!")

from google.colab import files
files.download('bm1.pkl')
#files.download('scaler.pkl')